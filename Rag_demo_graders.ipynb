{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce4af6b-75fe-4893-9482-c1f12f26afc1",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 32px; color: black;\">\n",
    "    Retrieval-Augmented Generation (RAG)\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711228f-7f85-4f45-975f-9e19b6c0c467",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    Retrieval-Augmented Generation (RAG) is a method that combines the two worlds of information retrieval and generative models. \n",
    "    In RAG, an information retrieval component fetches relevant documents or data based on a user query, and then this is passed down the pipeline to a language model which uses this retrieved information to generate a more informed, accurate, and contextually relevant response. \n",
    " RAG helps us overcome limitations of traditional language models, which may lack specific or up-to-date knowledge.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966b9db1-326b-438b-8584-551f8a7d2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\owner\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: opensearch-py in c:\\users\\owner\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.0 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opensearch-py) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opensearch-py) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opensearch-py) (2024.8.30)\n",
      "Requirement already satisfied: Events in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opensearch-py) (0.5)\n",
      "Requirement already satisfied: urllib3!=2.2.0,!=2.2.1,<3,>=1.26.19 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from opensearch-py) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\anaconda3\\lib\\site-packages (from python-dateutil->opensearch-py) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# pre-requisite installs \n",
    "!pip3 install requests\n",
    "!pip3 install opensearch-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903a2fa9-6ac8-4d06-86de-4e02965bddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from opensearchpy import OpenSearch\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import asyncio\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319d335-8d72-4a4f-93cd-1397e0e8eca5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    1st Step: Set Up a Connection with the OpenSearch Client 🧪\n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this first step, we establish a connection to the OpenSearch client, which is essential for querying and retrieving our data. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30afc7ad-b0d4-4971-b4d7-fc26c7cdb822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Password:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\opensearchpy\\connection\\http_urllib3.py:214: UserWarning: Connecting to https://149.165.153.78:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "password = getpass.getpass(f\"Password: \")\n",
    "client = OpenSearch(\n",
    "    hosts=[\"https://149.165.153.78:9200\"],  # dashboard is 5201 / opensearch indices at port 9200\n",
    "    http_auth=(\"admin\", password),\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3e970-a26d-40c1-bf95-81f563713e91",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    2nd Step: Connect to the Llama Model 🔗\n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this second step, we establish a connection to the Llama model. The Llama 3 model is hosted locally on LM Studio. By connecting to this model, we can leverage its capabilities to process retrieved information from OpenSearch.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25eb917c-fe77-4d81-a746-cc86f5d62516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the AnvilGPT API key:  ········\n"
     ]
    }
   ],
   "source": [
    "anvil_gpt_api_key = getpass.getpass(\"Enter the AnvilGPT API key: \")\n",
    "async def call_llama_model(query_payload):\n",
    "    llama_api_url = \"https://anvilgpt.rcac.purdue.edu/ollama/api/chat\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            llama_api_url,\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {anvil_gpt_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json=query_payload\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(\"Error fetching from Llama model:\", error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee3929-9557-4cc1-8e1e-d66fb09faabc",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    3rd Step: Query the OpenSearch Index with Data 🔍\n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this third step, we perform a query on the OpenSearch index to retrieve relevant data based on user input. By querying this index, we can filter and retrieve the specific documents or records that match the user's query.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9729d1a-7fb7-4d7c-a010-efbce82c20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get search results from OpenSearch\n",
    "async def get_search_results(user_query):\n",
    "    try:\n",
    "        response = client.search(\n",
    "            index=\"neo4j-elements\",\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"contents\": user_query  # Query based on user input\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response['hits']['hits']\n",
    "    except Exception as error:\n",
    "        print('Error connecting to OpenSearch:', error)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74b8a1-4506-4308-8fb1-109e78993be7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    4th Step: Process the User Query Through the Pipeline 🔄\n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this final step, the user query moves through the entire processing pipeline. \n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f7d3a38-9d13-429c-9696-59109624185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_user_input(user_query):\n",
    "    print(\"Fetching search results...\")\n",
    "    search_results = await get_search_results(user_query)\n",
    "\n",
    "    if not search_results:\n",
    "        print(\"No search results found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Preparing payload for Llama model...\")\n",
    "\n",
    "    formatted_results = '\\n'.join([\n",
    "        f\"Title: {hit['_source']['title']}\\n\"\n",
    "        f\"Content: {hit['_source']['contents']}\\n\"\n",
    "        f\"Contributor: {hit['_source']['contributor']}\\n\\n\"\n",
    "        for hit in search_results\n",
    "    ])\n",
    "\n",
    "    query_payload = {\n",
    "        \"model\": \"llama3:instruct\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant who helps summarize and organize information from search results.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"User Query: {user_query}\\nSearch Results:\\n{formatted_results}\"\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    print(\"Calling Llama model...\")\n",
    "    llama_response = await call_llama_model(query_payload)\n",
    "    print(llama_response)\n",
    "\n",
    "    if llama_response and 'choices' in llama_response and llama_response['choices']:\n",
    "        print(\"\\nLlama model response:\")\n",
    "        print(llama_response['choices'][0]['message']['content'])\n",
    "    else:\n",
    "        print(\"Unexpected response format or no choices available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444c212-267a-401b-a362-79853c300fc7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    Now try and ask the Model a Question 💬\n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    What is CyberGIS? What is geospatial data?\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42579c5-5bd2-444b-a79d-1c86113a9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the user query:  Give me some datasets about Chicago\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Enter the user query: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f8b44-1a38-49dd-b2e4-7b477bc325f4",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "Generating Results...\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f354fd29-98b5-4b32-bb5d-e66264c54054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host '149.165.153.78'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing payload for Llama model...\n",
      "Calling Llama model...\n",
      "{'model': 'llama3:instruct', 'created_at': '2024-11-12T17:29:00.974919041Z', 'message': {'role': 'assistant', 'content': \"Based on the search results, I've summarized some datasets related to Chicago:\\n\\n1. **Chicago Communities**: A geospatial dataset of communities within the city of Chicago (contributor: Rebecca (Becky) Vandewalle)\\n2. **Chicago Major Streets**: A geospatial dataset of major streets within the city of Chicago (contributor: Rebecca (Becky) Vandewalle)\\n\\nThese datasets might be useful for geographic information systems (GIS), urban planning, or community development projects.\\n\\nAdditionally, there are other results that may not directly relate to Chicago but could be relevant to GIS or geography-related topics:\\n\\n1. **Computer Science and Programming Courses in Geography Departments in the United States**: A study on how U.S. geography departments introduce computer science and programming skills in their degree programs (contributor: Forrest J. Bowlick)\\n2. **Map Projections**: An explanation of map projections, including mathematical models used to transform the Earth's surface onto a flat plane (contributor: Fangzheng Lyu)\\n\\nThese results could be useful for those interested in GIS, computer science, or geography.\\n\\nPlease let me know if you'd like me to summarize any specific topics or if you have further questions!\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 4678429894, 'load_duration': 2338066047, 'prompt_eval_count': 1294, 'prompt_eval_duration': 315908000, 'eval_count': 244, 'eval_duration': 1976201000}\n",
      "Unexpected response format or no choices available.\n"
     ]
    }
   ],
   "source": [
    "await handle_user_input(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81a1b6-0ea0-4b82-9bb2-397f2ad56017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae6aa024-6d93-4c5d-8820-31044172c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_payload(model, systemMessage, userMessage, stream):\n",
    "    query_payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": systemMessage\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": userMessage\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    return query_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7cdc193d-0d73-4b23-8088-09182ea2d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_documents(userQuery):\n",
    "    print(\"Fetching search results...\")\n",
    "    search_result = await get_search_results(user_query)\n",
    "\n",
    "    if not search_result:\n",
    "        print(\"No search results found.\")\n",
    "        return\n",
    "\n",
    "    documents = [\n",
    "        f\"Title: {hit['_source']['title']}\\n\"\n",
    "        f\"Content: {hit['_source']['contents']}\\n\"\n",
    "        f\"Contributor: {hit['_source']['contributor']}\\n\"\n",
    "        for hit in search_result\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "def extract_binary_score(content):\n",
    "    # Use regex to find the value of \"binary_score\" directly, even in incomplete JSON\n",
    "    match = re.search(r'\"binary_score\":\\s*\"(yes|no)\"', content)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the matched value (\"yes\" or \"no\")\n",
    "        binary_score = match.group(1)\n",
    "        return binary_score\n",
    "    else:\n",
    "        print(\"No valid binary_score found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35d0b6e2-1d75-43b9-9aba-eae43a096c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return only JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question. No reasoning or explination\"\"\"\n",
    "async def grade_documents(state):\n",
    "    question = state[\"question\"]\n",
    "    document_list = state[\"documents\"]\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\" \n",
    "    for d in document_list:\n",
    "        doc_grader_prompt_formated = doc_grader_prompt.format(document=d, question=question)\n",
    "        result = await call_llama_model(create_query_payload(\"llama3.2:latest\", doc_grader_instructions, doc_grader_prompt_formated, False))\n",
    "        #print(result)\n",
    "        grade = extract_binary_score(result['message']['content'])\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42a73629-d3b8-4b85-ad80-c32fbeab21e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host '149.165.153.78'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me some dataset about Chicago\"\n",
    "docs = await retrieve_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68f75792-4575-477b-8cec-f950b11a517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title: Computer Science and Programming Courses in Geography Departments in the United States\\nContent: Geographic information systems (GIS) are fundamental information technologies. The capabilities and applications of GIS continue to rapidly expand, requiring practitioners to have new skills and competencies, especially in computer science. There is little research, however, about how best to prepare the next generation of GIScientists with adequate computer science skills. This article explores how U.S. geography departments are introducing and developing computer science and programming skills in their geography and GIS degree programs. We review the degree requirements in fifty-five geography departments and discover that forty-four of them offer some kind of GIS programming course. Of the 210 separate degree options identified, however, only 22 require one of these courses for a degree. There is little consistency or emphasis on computer science and programming skills in geography or GIS degrees, despite the immense importance of these components in geography and GIS careers. We propose future research along distinct investigative tracks to build a research-based understanding of the educational interactions among GIS, computer science, programming, and geography.\\nContributor: Forrest J. Bowlick\\n',\n",
       " \"Title: Map Projections\\nContent: Map projections are mathematical models used to transform the curved Earth's surface onto a flat, two-dimensional plane. They are necessary because representing a three-dimensional spherical shape on a flat surface inevitably introduces some form of distortion. Different map projections are designed to preserve or minimize distortion of specific properties like area, shape, distance, or direction, depending on the intended application and geographic region.\\nContributor: Fangzheng Lyu\\n\",\n",
       " 'Title: Census tract annual averaged building loss in Texas\\nContent: This dataset is includes features related to Wildfire damage and loss to building in Texas. Original sources include: FEMA Hazards NRI datasets, WildfireRisk datasets, Historical Settlement Data Compilation (HISDAC-US) Dataset, Climate Engine datasets, Census tracts. \\nContributor: Zhenlei Song\\n',\n",
       " 'Title: Chicago Communitites\\nContent: This is a geospatial dataset of communities within the city of Chicago.\\nContributor: Rebecca (Becky) Vandewalle\\n',\n",
       " 'Title: Disparities of compound exposure of particulate matter (PM2. 5) and heat index using citywide monitoring networks\\nContent: As epicenters of environmental change, cities face multiple threats from environmental hazards, including poor air quality and extreme heat. The complex features of urban environments, such as varying landscapes and emission sources, significantly affect these phenomena. This study investigated the combined exposure to PM2.5 and extreme heat within Chicago, considering socioeconomic aspects to gage environmental justice issues. This study utilized high-resolution environmental datasets and Random Forest Spatial Interpolation (RFSI) to generate hourly Heat Index and PM2.5 maps at a spatial resolution of 250 m. The RFSI showed robust performance, with cross-validation R2 ranging from 0.78 to 0.98 (air temperature (Ta): 0.98, relative humidity (RH): 0.93, and PM2.5: 0.84) and RMSE from 0.84 to 5.4 (Ta: 1.09, RH: 5.40, and PM2.5: 0.84). With this result, this research visualized the spatial variations in extreme heat, as measured by the Heat Index and PM2.5 levels, and identified areas with critical exposure that are potentially harmful to the health of vulnerable populations. Furthermore, this study found the spatial disparities in exposure linked to socioeconomic factors by conducting a Welch ANOVA test. These findings can inform the development of targeted interventions considering the temporal-spatial disparities of heat and air pollution levels.\\nContributor: Yoonjung Ahn\\n',\n",
       " 'Title: Chicago Major Streets\\nContent: This is a geospatial dataset of major streets within the city of Chicago.\\nContributor: Rebecca (Becky) Vandewalle\\n',\n",
       " \"Title: 1916 Chicago Map\\nContent: Rand McNally & Co.'s new street number guide map of Chicago (1916) from the Harvard Map Collection.\\nContributor: Rebecca (Becky) Vandewalle\\n\",\n",
       " 'Title: Intermediate Results for Human Sentiments of Heat Exposure\\nContent: This data include example intermediate results for human sentiments of heat exposure at national-level at the US and city-level at the city of Chicago.\\nContributor: Fangzheng Lyu\\n',\n",
       " 'Title: Social Media (Twitter) Data Visualization\\nContent: This notebook provides examples of visualization of social media data including where the location-based Twitter data were posted in the City of Chicago and across the world.\\nContributor: Fangzheng Lyu\\n',\n",
       " 'Title: Geospatial Topological Relation Extraction from Text with Knowledge Augmentation\\nContent: Geospatial topological relation extraction (GeoTopoRE) aims to extract topological relations between named geospatial entities (i.e., geo-entities) in text. It is a domain-specific relation extraction (RE) task essential in geospatial knowledge graph construction and spatial reasoning. Unlike general-purpose RE, which primarily depends on semantic and syntactic cues, GeoTopoRE requires integrating geometric knowledge about geo-entities. This is essential for accurately capturing or inferring the complex geospatial relationships among entities. GeoTopoRE is not studied systematically and lacks dedicated datasets for evaluation, posing significant challenges to developing and assessing effective models. This study presents two major contributions: (i) the introduction of a high-quality, human-labeled dataset WikiTopo for the GeoTopoRE task, and (ii) a novel framework GeoWISE designed to adapt existing RE models to the GeoTopoRE task, with integrated semantic and external geospatial domain knowledge. We leverage coarse-to-fine-grained natural language inference (NLI) to align externally sourced knowledge with the semantic text context, enhanced by geospatial expertise. This integrated knowledge is then conveyed to language models as geospatial cues, enabling a nuanced understanding of topological relations. Empirical results demonstrate the efficacy of our framework in few-shot settings, showing significant and consistent improvements in the GeoTopoRE task for diverse state-of-the-art RE models.\\nContributor: Wei Hu\\n']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43e3836d-0dc1-4aa7-878d-36581f82f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n"
     ]
    }
   ],
   "source": [
    "grade_state = await grade_documents({\"documents\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b92abe9-a6e1-41ef-982d-b5eab43a92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_instructions = \"You are an assistant who helps summarize and organize information from search results.\"\n",
    "generation_prompt = \"\"\"User Query: {user_query}\\nSearch Results:\\n{formatted_results}\"\"\"\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc for doc in docs)\n",
    "async def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "    \n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(documents)\n",
    "    generation_prompt_formatted = generation_prompt.format(user_query=question, formatted_results=docs_txt)\n",
    "    llm_response = await call_llama_model(create_query_payload(\"llama3.2:latest\", generation_instructions, generation_prompt_formatted, False))\n",
    "    generation = llm_response['message']['content']\n",
    "    return {\"documents\": documents, \"generation\": generation, \"question\": question, \"loop_step\": loop_step+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf85d45f-319a-4712-b959-ad4c04f8b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATE---\n"
     ]
    }
   ],
   "source": [
    "generation_state = await generate(grade_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bae1d677-d166-4a2c-bc1d-15f5782a2fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the search results related to Chicago datasets:\n",
      "\n",
      "1. **Chicago Communities Dataset**\n",
      "\t* Source: Chicago Communitites ( contributed by Rebecca (Becky) Vandewalle)\n",
      "\t* Type: Geospatial dataset\n",
      "\t* Description: A geospatial dataset of communities within the city of Chicago.\n",
      "2. **Human Sentiments of Heat Exposure in Chicago**\n",
      "\t* Source: Intermediate Results for Human Sentiments of Heat Exposure\n",
      "\t* Contribution: Fangzheng Lyu\n",
      "\t* Type: Data analysis results\n",
      "\t* Description: Example intermediate results for human sentiments of heat exposure at national-level and city-level in the city of Chicago.\n",
      "3. **Social Media (Twitter) Data Visualization**\n",
      "\t* Source: Social Media (Twitter) Data Visualization\n",
      "\t* Contribution: Fangzheng Lyu\n",
      "\t* Type: Data visualization examples\n",
      "\t* Description: Examples of visualization of social media data, including location-based Twitter data for the City of Chicago and worldwide.\n",
      "\n",
      "Let me know if you'd like me to help with anything else!\n"
     ]
    }
   ],
   "source": [
    "print(generation_state[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "835cc10e-c625-426d-baaa-7f3d0cfc0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# Hallucination grader instructions \n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score. Return the json only without other explinations.\"\"\"\n",
    "\n",
    "### Answer Grader \n",
    "\n",
    "# Answer grader instructions \n",
    "answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "answer_grader_prompt = \"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score. Return the json only without additional explination\"\"\"\n",
    "\n",
    "\n",
    "async def grade_generation_v_documents_and_question(state, show_reason = False):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    max_retries = state.get(\"max_retries\", 3) # Default to 3 if not provided\n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=format_docs(documents), generation=generation)\n",
    "    result = await call_llama_model(create_query_payload(\"llama3.2:latest\", hallucination_grader_instructions, hallucination_grader_prompt_formatted, False))\n",
    "    if(show_reason == True):\n",
    "        print(result['message']['content'])\n",
    "    grade = extract_binary_score(result['message']['content'])\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        # Test using question and generation from above \n",
    "        answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=generation)\n",
    "        result = await call_llama_model(create_query_payload(\"llama3.2:latest\", answer_grader_instructions, answer_grader_prompt_formatted, False))\n",
    "        if(show_reason == True):\n",
    "            print(result['message']['content'])\n",
    "        grade = extract_binary_score(result['message']['content'])\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif state[\"loop_step\"] <= max_retries:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "            return \"max retries\"  \n",
    "    elif state[\"loop_step\"] <= max_retries:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "        return \"max retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8773b615-9684-4242-be8b-3487290b4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "{\n",
      "  \"binary_score\": \"yes\",\n",
      "  \"explanation\": \"The student answer accurately summarizes all three datasets and their contributors, ensuring it is grounded in the provided FACTS.\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "```\n",
      "{\n",
      "  \"binary_score\": \"yes\",\n",
      "  \"explanation\": \"The student answer provides specific and relevant dataset information about Chicago, including geospatial data, human sentiments, and social media data. The datasets are not just listed but also include metadata (creator names) which is additional context.\"\n",
      "}\n",
      "```\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n"
     ]
    }
   ],
   "source": [
    "generation_state = await generate(grade_state)\n",
    "verdict = await grade_generation_v_documents_and_question(generation_state, show_reason = True)\n",
    "while verdict != \"useful\":\n",
    "    if verdict == \"not supported\" or verdict == \"not useful\":\n",
    "        generation_state = await generate(generation_state)\n",
    "        verdict = await grade_generation_v_documents_and_question(generation_state, show_reason = True)\n",
    "    elif verdict == \"max retries\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "952fde20-7375-4c0c-a85d-1b837cd09126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'useful'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c0bbb-571e-4bc2-a731-4b9029f8d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
