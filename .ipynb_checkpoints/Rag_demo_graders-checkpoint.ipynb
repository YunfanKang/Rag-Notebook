{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce4af6b-75fe-4893-9482-c1f12f26afc1",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 32px; color: black;\">\n",
    "    Retrieval-Augmented Generation (RAG)\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711228f-7f85-4f45-975f-9e19b6c0c467",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    Retrieval-Augmented Generation (RAG) is a method that combines the two worlds of information retrieval and generative models. \n",
    "    In RAG, an information retrieval component fetches relevant documents or data based on a user query, and then this is passed down the pipeline to a language model which uses this retrieved information to generate a more informed, accurate, and contextually relevant response. \n",
    " RAG helps us overcome limitations of traditional language models, which may lack specific or up-to-date knowledge.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "966b9db1-326b-438b-8584-551f8a7d2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Collecting opensearch-py\n",
      "  Downloading opensearch_py-2.7.1-py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m904.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.32.0 (from opensearch-py)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from opensearch-py) (2.8.2)\n",
      "Collecting certifi>=2024.07.04 (from opensearch-py)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting Events (from opensearch-py)\n",
      "  Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,!=2.2.1,<3,>=1.26.19 in /opt/conda/lib/python3.11/site-packages (from opensearch-py) (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->opensearch-py) (1.16.0)\n",
      "Installing collected packages: Events, certifi, requests, opensearch-py\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "Successfully installed Events-0.5 certifi-2024.8.30 opensearch-py-2.7.1 requests-2.32.3\n"
     ]
    }
   ],
   "source": [
    "# pre-requisite installs \n",
    "!pip3 install requests\n",
    "!pip3 install opensearch-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903a2fa9-6ac8-4d06-86de-4e02965bddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from opensearchpy import OpenSearch\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import asyncio\n",
    "import re\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319d335-8d72-4a4f-93cd-1397e0e8eca5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    1st Step: Set Up a Connection with the OpenSearch Client И\n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this first step, we establish a connection to the OpenSearch client, which is essential for querying and retrieving our data. \n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30afc7ad-b0d4-4971-b4d7-fc26c7cdb822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Password:  路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "password = getpass.getpass(f\"Password: \")\n",
    "client = OpenSearch(\n",
    "    hosts=[\"https://149.165.153.78:9200\"],  # dashboard is 5201 / opensearch indices at port 9200\n",
    "    http_auth=(\"admin\", password),\n",
    "    verify_certs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3e970-a26d-40c1-bf95-81f563713e91",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    2nd Step: Connect to the Llama Model \n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this second step, we establish a connection to the Llama model. The Llama 3 model is hosted locally on LM Studio. By connecting to this model, we can leverage its capabilities to process retrieved information from OpenSearch.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25eb917c-fe77-4d81-a746-cc86f5d62516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the AnvilGPT API key:  路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "anvil_gpt_api_key = getpass.getpass(\"Enter the AnvilGPT API key: \")\n",
    "async def call_llama_model(query_payload):\n",
    "    llama_api_url = \"https://anvilgpt.rcac.purdue.edu/ollama/api/chat\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            llama_api_url,\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {anvil_gpt_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json=query_payload\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(\"Error fetching from Llama model:\", error)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee3929-9557-4cc1-8e1e-d66fb09faabc",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    3rd Step: Query the OpenSearch Index with Data \n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this third step, we perform a query on the OpenSearch index to retrieve relevant data based on user input. By querying this index, we can filter and retrieve the specific documents or records that match the user's query.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9729d1a-7fb7-4d7c-a010-efbce82c20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get search results from OpenSearch\n",
    "async def get_search_results(user_query):\n",
    "    try:\n",
    "        response = client.search(\n",
    "            index=\"neo4j-elements\",\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"contents\": user_query  # Query based on user input\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return response['hits']['hits']\n",
    "    except Exception as error:\n",
    "        print('Error connecting to OpenSearch:', error)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74b8a1-4506-4308-8fb1-109e78993be7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    4th Step: Process the User Query Through the Pipeline \n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    In this final step, the user query moves through the entire processing pipeline. \n",
    "\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f7d3a38-9d13-429c-9696-59109624185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_user_input(user_query):\n",
    "    print(\"Fetching search results...\")\n",
    "    search_results = await get_search_results(user_query)\n",
    "\n",
    "    if not search_results:\n",
    "        print(\"No search results found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Preparing payload for Llama model...\")\n",
    "\n",
    "    formatted_results = '\\n'.join([\n",
    "        f\"Title: {hit['_source']['title']}\\n\"\n",
    "        f\"Content: {hit['_source']['contents']}\\n\"\n",
    "        f\"Contributor: {hit['_source']['contributor']}\\n\\n\"\n",
    "        for hit in search_results\n",
    "    ])\n",
    "\n",
    "    query_payload = {\n",
    "        \"model\": \"llama3:instruct\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant who helps summarize and organize information from search results.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"User Query: {user_query}\\nSearch Results:\\n{formatted_results}\"\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    print(\"Calling Llama model...\")\n",
    "    llama_response = await call_llama_model(query_payload)\n",
    "    print(llama_response)\n",
    "\n",
    "    if llama_response and 'choices' in llama_response and llama_response['choices']:\n",
    "        print(\"\\nLlama model response:\")\n",
    "        print(llama_response['choices'][0]['message']['content'])\n",
    "    else:\n",
    "        print(\"Unexpected response format or no choices available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444c212-267a-401b-a362-79853c300fc7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "    Now try and ask the Model a Question \n",
    "</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; font-family: Arial, sans-serif; color: #333;\">\n",
    "    What is CyberGIS? What is geospatial data?\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42579c5-5bd2-444b-a79d-1c86113a9f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the user query:  What is CyberGIS? \n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Enter the user query: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f8b44-1a38-49dd-b2e4-7b477bc325f4",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Helvetica Neue', Arial, sans-serif; font-size: 20px; color: black;\">\n",
    "Generating Results...\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f354fd29-98b5-4b32-bb5d-e66264c54054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching search results...\n",
      "Preparing payload for Llama model...\n",
      "Calling Llama model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host '149.165.153.78'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3:instruct', 'created_at': '2024-11-12T17:59:26.755075381Z', 'message': {'role': 'assistant', 'content': 'CyberGIS (Cyberinfrastructure for Geospatial Information Systems) is a framework that enables the integration of domain-specific models with High-Performance Computing (HPC) resources. It aims to democratize access to HPC resources and enable domain scientists in various fields, including social and environmental sciences, to solve computationally intensive problems.\\n\\nCyberGIS provides a middleware toolkit called CyberGIS-Compute, which allows domain experts to create containerized software environments for their models without requiring extensive technical knowledge of HPC or containerization technologies. This enables users to execute their models on HPC resources, such as science gateways, and provides consistent software environments across different computing platforms.\\n\\nSome key features of CyberGIS include:\\n\\n1. Containerization: CyberGIS-Compute uses containerization technologies like Docker and Singularity to create isolated software environments for domain-specific models.\\n2. Middleware: CyberGIS acts as a middleware layer between HPC resources and the domain-specific models, simplifying the process of integrating models with HPC resources.\\n3. Science gateways: CyberGIS integrates with science gateways, which are platforms that provide access to computational resources and facilitate collaboration among researchers in various fields.\\n4. Reproducibility: CyberGIS enables reproducibility by providing a consistent software environment for domain-specific models, making it easier to replicate results and collaborate with others.\\n\\nCyberGIS has been applied to various fields, including emergency evacuation simulation, spatial agent-based modeling, precision agriculture, and more. It has the potential to enable researchers in different disciplines to work together more effectively and solve complex problems that require large-scale computational resources.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 5071187680, 'load_duration': 2150851193, 'prompt_eval_count': 1036, 'prompt_eval_duration': 253936000, 'eval_count': 324, 'eval_duration': 2573821000}\n",
      "Unexpected response format or no choices available.\n"
     ]
    }
   ],
   "source": [
    "await handle_user_input(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d02f42-80d2-47c1-9abf-ff5ccae9b52b",
   "metadata": {},
   "source": [
    "![langgraph_adaptive_rag.png](langgraph_adaptive_rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae6aa024-6d93-4c5d-8820-31044172c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_payload(model, systemMessage, userMessage, stream):\n",
    "    query_payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": systemMessage\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": userMessage\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    return query_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cdc193d-0d73-4b23-8088-09182ea2d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_documents(userQuery):\n",
    "    print(\"Fetching search results...\")\n",
    "    search_result = await get_search_results(userQuery)\n",
    "\n",
    "    if not search_result:\n",
    "        print(\"No search results found.\")\n",
    "        return\n",
    "\n",
    "    documents = [\n",
    "        f\"Title: {hit['_source']['title']}\\n\"\n",
    "        f\"Content: {hit['_source']['contents']}\\n\"\n",
    "        f\"Contributor: {hit['_source']['contributor']}\\n\"\n",
    "        for hit in search_result\n",
    "    ]\n",
    "    return documents\n",
    "\n",
    "def extract_binary_score(content):\n",
    "    # Use regex to find the value of \"binary_score\" directly, even in incomplete JSON\n",
    "    match = re.search(r'\"binary_score\":\\s*\"(yes|no)\"', content)\n",
    "    \n",
    "    if match:\n",
    "        # Extract the matched value (\"yes\" or \"no\")\n",
    "        binary_score = match.group(1)\n",
    "        return binary_score\n",
    "    else:\n",
    "        print(\"No valid binary_score found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb87ebc-a74a-4203-be0e-462f98774a6f",
   "metadata": {},
   "source": [
    "### Document Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35d0b6e2-1d75-43b9-9aba-eae43a096c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return only JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question. No reasoning or explination\"\"\"\n",
    "async def grade_documents(state):\n",
    "    question = state[\"question\"]\n",
    "    document_list = state[\"documents\"]\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\" \n",
    "    for d in document_list:\n",
    "        doc_grader_prompt_formated = doc_grader_prompt.format(document=d, question=question)\n",
    "        result = await call_llama_model(create_query_payload(\"llama3.2:latest\", doc_grader_instructions, doc_grader_prompt_formated, False))\n",
    "        #print(result)\n",
    "        grade = extract_binary_score(result['message']['content'])\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42a73629-d3b8-4b85-ad80-c32fbeab21e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host '149.165.153.78'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me some dataset about Chicago\"\n",
    "docs = await retrieve_documents(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68f75792-4575-477b-8cec-f950b11a517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title: Chicago Communitites\\nContent: This is a geospatial dataset of communities within the city of Chicago.\\nContributor: Rebecca (Becky) Vandewalle\\n',\n",
       " 'Title: Chicago Major Streets\\nContent: This is a geospatial dataset of major streets within the city of Chicago.\\nContributor: Rebecca (Becky) Vandewalle\\n',\n",
       " 'Title: Large-Scale Weather Event Dataset\\nContent: LSTW is a large-scale, country-wide dataset for transportation and traffic research, which contains traffic and weather event data for the United States. In terms of weather events, they have several types including rain, snow, storm, cold weather event, etc. This dataset is continuously being collected from August 2016, and today it contains about 37 million traffic and weather events.\\nContributor: Wei Hu\\n',\n",
       " \"Title: Census tract level Chicago Boundary Shapefile\\nContent: The cartographic boundary files are simplified representations of selected geographic areas from the Census Bureau's MAF/TIGER geographic database. These boundary files are specifically designed for small scale thematic mapping. The cartographic boundary files are available in shapefile. A shapefile is a geospatial data format for use in geographic information system (GIS) software. This dataset includes boundary shapefiles for the Chicago, US region at census tract level.\\nContributor: Fangzheng Lyu\\n\",\n",
       " 'Title: Computer Science and Programming Courses in Geography Departments in the United States\\nContent: Geographic information systems (GIS) are fundamental information technologies. The capabilities and applications of GIS continue to rapidly expand, requiring practitioners to have new skills and competencies, especially in computer science. There is little research, however, about how best to prepare the next generation of GIScientists with adequate computer science skills. This article explores how U.S. geography departments are introducing and developing computer science and programming skills in their geography and GIS degree programs. We review the degree requirements in fifty-five geography departments and discover that forty-four of them offer some kind of GIS programming course. Of the 210 separate degree options identified, however, only 22 require one of these courses for a degree. There is little consistency or emphasis on computer science and programming skills in geography or GIS degrees, despite the immense importance of these components in geography and GIS careers. We propose future research along distinct investigative tracks to build a research-based understanding of the educational interactions among GIS, computer science, programming, and geography.\\nContributor: Forrest J. Bowlick\\n',\n",
       " 'Title: Globe-LFMC 2.0, an enhanced and updated dataset for live fuel moisture content research\\nContent: Globe-LFMC 2.0, an updated version of Globe-LFMC, is a comprehensive dataset of over 280,000 Live Fuel Moisture Content (LFMC) measurements. These measurements were gathered through field campaigns conducted in 15 countries spanning 47 years. In contrast to its prior version, Globe-LFMC 2.0 incorporates over 120,000 additional data entries, introduces more than 800 new sampling sites, and comprises LFMC values obtained from samples collected until the calendar year 2023. Each entry within the dataset provides essential information, including date, geographical coordinates, plant species, functional type, and, where available, topographical details. Moreover, the dataset encompasses insights into the sampling and weighing procedures, as well as information about land cover type and meteorological conditions at the time and location of each sampling event. Globe-LFMC 2.0 can facilitate advanced LFMC research, supporting studies on wildfire behaviour, physiological traits, ecological dynamics, and land surface modelling, whether remote sensing-based or otherwise. This dataset represents a valuable resource for researchers exploring the diverse LFMC aspects, contributing to the broader field of environmental and ecological research.\\nContributor: Yi Qi\\n',\n",
       " 'Title: Projecting Responses of Ecological Diversity In Changing Terrestrial Systems (PREDICTS)\\nContent: This is an updated version of the 2016 release of the PREDICTS database.\\n\\nData review in 2021-2023 has resulted in some changes and additions to the database. One source (consisting of three studies) has changed ID. 24 studies have an additional blocking structure that was mistakenly omitted in the original database release. Three studies have additional blocks and records, as data from additional years have been included in this extract. We recommend using this updated version of these data, rather than the original 2016 release.\\n\\nA dataset of 3,278,056 measurement, collated from 26,194 sampling locations in 94 countries and representing 47,089 species. The data were collated from 480 existing spatial comparisons of local-scale biodiversity exposed to different intensities and types of anthropogenic pressures, from terrestrial sites around the world. The database was assembled as part of the PREDICTS project - Projecting Responses of Ecological Diversity In Changing Terrestrial Systems.\\n\\nThe taxonomic identifications provided in the original data sets are those determined at the time of the original research, and so will not reflect subsequent taxonomic changes.\\nContributor: Logan Hysen\\n',\n",
       " \"Title: Map Projections\\nContent: Map projections are mathematical models used to transform the curved Earth's surface onto a flat, two-dimensional plane. They are necessary because representing a three-dimensional spherical shape on a flat surface inevitably introduces some form of distortion. Different map projections are designed to preserve or minimize distortion of specific properties like area, shape, distance, or direction, depending on the intended application and geographic region.\\nContributor: Fangzheng Lyu\\n\",\n",
       " \"Title: 1916 Chicago Map\\nContent: Rand McNally & Co.'s new street number guide map of Chicago (1916) from the Harvard Map Collection.\\nContributor: Rebecca (Becky) Vandewalle\\n\",\n",
       " 'Title: Intermediate Results for Human Sentiments of Heat Exposure\\nContent: This data include example intermediate results for human sentiments of heat exposure at national-level at the US and city-level at the city of Chicago.\\nContributor: Fangzheng Lyu\\n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43e3836d-0dc1-4aa7-878d-36581f82f9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n"
     ]
    }
   ],
   "source": [
    "grade_state = await grade_documents({\"documents\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a5d48-ebe2-48f7-a055-d1ed74423b56",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b92abe9-a6e1-41ef-982d-b5eab43a92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_instructions = \"You are an assistant who helps summarize and organize information from search results.\"\n",
    "generation_prompt = \"\"\"User Query: {user_query}\\nSearch Results:\\n{formatted_results}\"\"\"\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc for doc in docs)\n",
    "async def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "    \n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(documents)\n",
    "    generation_prompt_formatted = generation_prompt.format(user_query=question, formatted_results=docs_txt)\n",
    "    llm_response = await call_llama_model(create_query_payload(\"llama3.2:latest\", generation_instructions, generation_prompt_formatted, False))\n",
    "    generation = llm_response['message']['content']\n",
    "    return {\"documents\": documents, \"generation\": generation, \"question\": question, \"loop_step\": loop_step+1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf85d45f-319a-4712-b959-ad4c04f8b972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATE---\n"
     ]
    }
   ],
   "source": [
    "generation_state = await generate(grade_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bae1d677-d166-4a2c-bc1d-15f5782a2fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the search results related to Chicago datasets:\n",
      "\n",
      "1. **Chicago Communities Dataset**\n",
      "\t* Source: Chicago Communitites ( contributed by Rebecca (Becky) Vandewalle)\n",
      "\t* Type: Geospatial dataset\n",
      "\t* Description: A geospatial dataset of communities within the city of Chicago.\n",
      "2. **Human Sentiments of Heat Exposure in Chicago**\n",
      "\t* Source: Intermediate Results for Human Sentiments of Heat Exposure\n",
      "\t* Contribution: Fangzheng Lyu\n",
      "\t* Type: Data analysis results\n",
      "\t* Description: Example intermediate results for human sentiments of heat exposure at national-level and city-level in the city of Chicago.\n",
      "3. **Social Media (Twitter) Data Visualization**\n",
      "\t* Source: Social Media (Twitter) Data Visualization\n",
      "\t* Contribution: Fangzheng Lyu\n",
      "\t* Type: Data visualization examples\n",
      "\t* Description: Examples of visualization of social media data, including location-based Twitter data for the City of Chicago and worldwide.\n",
      "\n",
      "Let me know if you'd like me to help with anything else!\n"
     ]
    }
   ],
   "source": [
    "print(generation_state[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c8684-21fb-4f87-8abc-79bdc9b6f8a4",
   "metadata": {},
   "source": [
    "### Hallucination Grader and Answer Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "835cc10e-c625-426d-baaa-7f3d0cfc0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader \n",
    "\n",
    "# Hallucination grader instructions \n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score. Return the json only without other explinations.\"\"\"\n",
    "\n",
    "### Answer Grader \n",
    "\n",
    "# Answer grader instructions \n",
    "answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "answer_grader_prompt = \"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score. Return the json only without additional explination\"\"\"\n",
    "\n",
    "\n",
    "async def grade_generation_v_documents_and_question(state, show_reason = False):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    max_retries = state.get(\"max_retries\", 3) # Default to 3 if not provided\n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=format_docs(documents), generation=generation)\n",
    "    result = await call_llama_model(create_query_payload(\"llama3.2:latest\", hallucination_grader_instructions, hallucination_grader_prompt_formatted, False))\n",
    "    if(show_reason == True):\n",
    "        print(result['message']['content'])\n",
    "    grade = extract_binary_score(result['message']['content'])\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        # Test using question and generation from above \n",
    "        answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=generation)\n",
    "        result = await call_llama_model(create_query_payload(\"llama3.2:latest\", answer_grader_instructions, answer_grader_prompt_formatted, False))\n",
    "        if(show_reason == True):\n",
    "            print(result['message']['content'])\n",
    "        grade = extract_binary_score(result['message']['content'])\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif state[\"loop_step\"] <= max_retries:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "            return \"max retries\"  \n",
    "    elif state[\"loop_step\"] <= max_retries:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "        return \"max retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8773b615-9684-4242-be8b-3487290b4105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "{\n",
      "  \"binary_score\": \"yes\",\n",
      "  \"explanation\": \"The student answer accurately summarizes all three datasets and their contributors, ensuring it is grounded in the provided FACTS.\"\n",
      "}\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "```\n",
      "{\n",
      "  \"binary_score\": \"yes\",\n",
      "  \"explanation\": \"The student answer provides specific and relevant dataset information about Chicago, including geospatial data, human sentiments, and social media data. The datasets are not just listed but also include metadata (creator names) which is additional context.\"\n",
      "}\n",
      "```\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n"
     ]
    }
   ],
   "source": [
    "generation_state = await generate(grade_state)\n",
    "verdict = await grade_generation_v_documents_and_question(generation_state, show_reason = True)\n",
    "while verdict != \"useful\":\n",
    "    if verdict == \"not supported\" or verdict == \"not useful\":\n",
    "        generation_state = await generate(generation_state)\n",
    "        verdict = await grade_generation_v_documents_and_question(generation_state, show_reason = True)\n",
    "    elif verdict == \"max retries\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c0bbb-571e-4bc2-a731-4b9029f8d5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
